# -*- coding: utf-8 -*-
"""Image stitching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15NcZ2z6qNgG3IgnflYWpDBS1oRu6MJH6
"""

! pip install opencv-contrib-python==3.4.2.16

import cv2
cv2.__version__

import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow

img_right = cv2.imread('/content/Stiching_right.png')


img_left = cv2.imread('/content/Stithing_left_change.png')

plt.figure(figsize=(30,20))

plt.subplot(1,2,1)
plt.title("Left Image")
plt.imshow(img_left)

plt.subplot(1,2,2)
plt.imshow(img_right)
plt.title("Right Image")

plt.tight_layout()

def fixColor(image):
    return(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

img1 = cv2.cvtColor(img_right,cv2.COLOR_BGR2GRAY)
img2 = cv2.cvtColor(img_left,cv2.COLOR_BGR2GRAY)

plt.figure(figsize=(30,20))

plt.subplot(2,2,1)
plt.title("Left Image")
plt.imshow(fixColor(img_left))

plt.subplot(2,2,2)
plt.title("Grayscale of Left Image")
plt.imshow(img2)

plt.subplot(2,2,3)
plt.title("Right Image")
plt.imshow(fixColor(img_right))

plt.subplot(2,2,4)
plt.title("Grayscale of Right Image")
plt.imshow(img1)

plt.tight_layout()

orb = cv2.ORB_create(2500)

kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)

len(kp1)

img_right_kp = cv2.drawKeypoints(img_right, kp1, np.array([]), color=(0,0, 255))
img_left_kp = cv2.drawKeypoints(img_left, kp2, None)

plt.figure(figsize=(30,20))
plt.subplot(1,2,1)
plt.imshow(fixColor(img_left_kp ))

plt.subplot(1,2,2)
plt.imshow(fixColor(img_right_kp ))
plt.tight_layout()

type(img_left_kp)

bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1,des2,)

matches = sorted(matches, key = lambda x:x.distance)

draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color
                   flags = 2)

matched_features_image = cv2.drawMatches(fixColor(img_right), kp1, fixColor(img_left), kp2, matches[:30], None,**draw_params)

plt.figure(figsize=(30,20))
plt.imshow(matched_features_image)

if len(matches) >= 4:
    src = np.float32([ kp1[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)
    dst = np.float32([ kp2[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)

    H, masked = cv2.findHomography(src, dst, cv2.RANSAC, 5.0)
else:
    raise AssertionError("Can't find enough keypoints.")

dst = cv2.warpPerspective(img_right,H,(img_left.shape[1] + img_right.shape[1], img_left.shape[0]))

plt.figure(figsize=(30,20))
plt.title('Warped Image')
plt.imshow(fixColor(dst))

dst[0:img_left.shape[0], 0:img_left.shape[1]] = img_left

cv2.imwrite('resultant_stitched_panorama.jpg',dst)

plt.figure(figsize=(30,20))
plt.title('Stitched Image')
plt.imshow(fixColor(dst))

plt.imshow(img1)

